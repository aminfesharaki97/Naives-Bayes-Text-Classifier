{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/datascience/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/datascience/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# Functions\n",
    "def clean_tokenize(words):\n",
    "    # Remove punctuation characters\n",
    "    cleaned = words.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    cleaned = cleaned.lower()\n",
    "    # Tokenize the text\n",
    "    tokenized = cleaned.split()\n",
    "    # Remove stopwords\n",
    "    tokenized = [word for word in tokenized if word not in sw]\n",
    "    return tokenized\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "def get_patterns(text, num_words):\n",
    "    \n",
    "    top_words = []\n",
    "    text = token_normal(text)\n",
    " \n",
    "    total_tokens = len(text)\n",
    "    unique_tokens = len(set(text))\n",
    "    avg_token_len = np.mean([len(w) for w in text])\n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "\n",
    "    top_words = FreqDist(text).most_common(num_words)\n",
    "    results = {\"tokens\": total_tokens,\n",
    "              \"unique_tokens\": unique_tokens,\n",
    "              \"avg_token_len\":avg_token_len,\n",
    "              \"lex_diversity\": lex_diversity,\n",
    "              \"top_words\":top_words}\n",
    "    return results\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    text = re.findall(r'\\b\\w+\\b|#\\w+', text)\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"/Users/datascience/Desktop/Text Mining/Datasets/2020_Conventions.db\") \n",
    "convention_cur = convention_db.cursor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            ''' \n",
    "                            SELECT text, party \n",
    "                            FROM conventions\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "    text = row[0] # Return Text\n",
    "    party = row[1] # Return Party\n",
    "    \n",
    "    ## Clean and tokenize the text \n",
    "    \n",
    "    # Make Text lower case\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove Stop Words\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    # Join tokenized words into a single string\n",
    "    cleaned_text = ' '.join(tokens) \n",
    "    # Append Data together\n",
    "    convention_data.append([cleaned_text, party]) # Append Data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skip content company careers press freelancers...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calling full session quadrennial national conv...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every four years come together reaffirm democr...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fight perfect union fighting soul country live...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>must come together defeat donald trump elect j...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>come together decry darkness light way forward...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>way see big problems demand big solutions love...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>thing single american fix country alone even p...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>greetings archbishop elpidophoros greek orthod...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>concludes convention program evening stand rec...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       party\n",
       "0     skip content company careers press freelancers...  Democratic\n",
       "1     calling full session quadrennial national conv...  Democratic\n",
       "2     every four years come together reaffirm democr...  Democratic\n",
       "3     fight perfect union fighting soul country live...  Democratic\n",
       "4     must come together defeat donald trump elect j...  Democratic\n",
       "...                                                 ...         ...\n",
       "2536  come together decry darkness light way forward...  Democratic\n",
       "2537  way see big problems demand big solutions love...  Democratic\n",
       "2538  thing single american fix country alone even p...  Democratic\n",
       "2539  greetings archbishop elpidophoros greek orthod...  Democratic\n",
       "2540  concludes convention program evening stand rec...  Democratic\n",
       "\n",
       "[2541 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with tokenized text in a single string and party identification\n",
    "convention_df = pd.DataFrame(convention_data, columns=[\"text\", \"party\"])\n",
    "convention_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['would say knew joe biden going woman vice president', 'Democratic'],\n",
       " ['american people forever indebted work front line', 'Democratic'],\n",
       " ['record breaking number women ran midterms', 'Democratic'],\n",
       " ['alarm went morning june real violent night louis four officers shot others hit rocks fireworks least five businesses damaged looted set fire officer wellness coordinator cit coordinator police department keenly aware rioting spent evening getting ready mobilize support officers impacted',\n",
       "  'Republican'],\n",
       " ['land greatness', 'Republican'],\n",
       " ['maryland', 'Republican'],\n",
       " ['state sovereignty', 'Republican'],\n",
       " ['ladies gentlemen leaders fighters freedom liberty american dream best yet come',\n",
       "  'Republican'],\n",
       " ['surprised happy', 'Democratic'],\n",
       " ['immediately upon taking office president trump changed things change threatened establishment establishment fought back democrat obstruction phony investigations dishonest media incredible stories negative president seen anything like despite everything threw president trump delivered american people delivered like never building strongest economy american history million new jobs lowest ever unemployment black hispanic americans ending biden era lopsided trade deals sent factories overseas passing historic usmca taking china winning trade war protecting strengthening medicare social security lowering cost insulin delivering first real drop drug prices years restoring military fixing va bringing troops home taking world deadliest terrorists battlefield soleimani baghdadi isis brought justice',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2236 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text, fw): \n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize an empty dictionary to store the feature words\n",
    "    ret_dict = dict()\n",
    "    # Initialize a set to keep track of the words that have already been seen\n",
    "    seen_words = set()\n",
    "    # Split the text into a list of words\n",
    "    words = text.split()\n",
    "    # Iterate over the words in the text\n",
    "    for word in words:\n",
    "        # Check if the word is in the feature words and if it hasn't been seen before\n",
    "        if word in fw and word not in seen_words:\n",
    "            # If the word is in the feature words and hasn't been seen before, add it to the return dictionary\n",
    "            ret_dict[word] = True\n",
    "            # Add the word to the set of seen words\n",
    "            seen_words.add(word)\n",
    "    # Return the dictionary of feature words\n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': True}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick Test to see if the function works\n",
    "conv_features(\"this is a test\",feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fast': True, 'brown': True}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick Test to see if the function works\n",
    "conv_features(\"fast fast brown fox\",feature_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conv_features seems to work as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.494\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     27.1 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     15.8 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Poise About the Classifier\n",
    "\n",
    "The \"Most Informative Features\" section displays the most significant words or phrases the classifier takes into account while making its predictions. For each feature, the results show the ratio of the likelihood of the feature appearing in documents classified as \"Republican\" compared to \"Democrat\". For instance, if the feature \"china = True\" is present, it is 27.1 times more likely to appear in documents classified as \"Republican\" rather than in documents classified as \"Democrat\".\n",
    "\n",
    "### My Observations\n",
    "The results indicate that certain words, like \"china,\" \"votes,\" \"enforcement,\" \"destroy,\" and \"freedoms,\" play a significant role in determining political party affiliation. These words are up to 27 times more likely to show up in Republican or Democratic text. Other terms like \"climate,\" \"crime,\" and \"media\" also have significant ratios, but not as strong. On the other hand, there are words such as \"beliefs,\" \"defense,\" \"isis,\" and \"trade,\" that have an equal probability of appearing in both Republican and Democratic text, with a ratio of 13:1.\n",
    "\n",
    "More specifically, it can be observed that the topic of climate is more likely to appear in texts associated with the Democratic party, which aligns with the party's known stance on the issue. However, the results also indicate an imbalance in the dataset as the majority of the top informative features are associated with the Republican party.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"/Users/datascience/Desktop/Text Mining/Datasets/congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "for row in results :\n",
    "    text = row[0] # Return Text\n",
    "    party = row[1] # Return Party\n",
    "    \n",
    "    ## Clean and tokenize the text \n",
    "    \n",
    "    # Make Text lower case\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove Stop Words\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    # Join tokenized words into a single string\n",
    "    cleaned_text = ' '.join(tokens) \n",
    "    # Append Data together\n",
    "    tweet_data.append([cleaned_text, party]) # Append Data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skip content company careers press freelancers...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calling full session quadrennial national conv...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every four years come together reaffirm democr...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fight perfect union fighting soul country live...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>must come together defeat donald trump elect j...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>come together decry darkness light way forward...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>way see big problems demand big solutions love...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>thing single american fix country alone even p...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>greetings archbishop elpidophoros greek orthod...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>concludes convention program evening stand rec...</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet       party\n",
       "0     skip content company careers press freelancers...  Democratic\n",
       "1     calling full session quadrennial national conv...  Democratic\n",
       "2     every four years come together reaffirm democr...  Democratic\n",
       "3     fight perfect union fighting soul country live...  Democratic\n",
       "4     must come together defeat donald trump elect j...  Democratic\n",
       "...                                                 ...         ...\n",
       "2536  come together decry darkness light way forward...  Democratic\n",
       "2537  way see big problems demand big solutions love...  Democratic\n",
       "2538  thing single american fix country alone even p...  Democratic\n",
       "2539  greetings archbishop elpidophoros greek orthod...  Democratic\n",
       "2540  concludes convention program evening stand rec...  Democratic\n",
       "\n",
       "[2541 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with tokenized text in a single string and party identification\n",
    "tweet_df = pd.DataFrame(convention_data, columns=[\"tweet\", \"party\"])\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jimmy panetta', 'Democratic'],\n",
       " ['marcy kaptur', 'Democratic'],\n",
       " ['debbie wasserman schultz', 'Democratic'],\n",
       " ['dave brat', 'Republican'],\n",
       " ['antonio sabàto jr', 'Republican'],\n",
       " ['marcia fudge', 'Democratic'],\n",
       " ['scott peters', 'Democratic'],\n",
       " ['mariah phillips', 'Democratic'],\n",
       " ['jimmy panetta', 'Democratic'],\n",
       " ['lucille', 'Democratic']]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: jimmy panetta\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: marcy kaptur\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: debbie wasserman schultz\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: dave brat\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: antonio sabàto jr\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "The classifier got it wrong.\n",
      "\n",
      "Here's our (cleaned) tweet: marcia fudge\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: scott peters\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: mariah phillips\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: jimmy panetta\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n",
      "Here's our (cleaned) tweet: lucille\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "The classifier got it right!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample :\n",
    "    # Classifier to predict party\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "\n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    if estimated_party == party:\n",
    "        print(\"The classifier got it right!\")\n",
    "    else:\n",
    "        print(\"The classifier got it wrong.\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy of the classifier is: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Print overall accuracy of the classifier \n",
    "actual_parties = [party for tweet, party in tweet_data_sample]\n",
    "estimated_parties = [classifier.classify(conv_features(tweet, feature_words)) \n",
    "                     for tweet, party in tweet_data_sample]\n",
    "\n",
    "accuracy = sum(actual == estimated for actual, estimated \n",
    "               in zip(actual_parties, estimated_parties)) / len(tweet_data_sample)\n",
    "\n",
    "print(f\"The overall accuracy of the classifier is: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = ['Republican', 'Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties:\n",
    "    for p1 in parties:\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, party = tp    \n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 656, 'Democratic': 3726}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 692, 'Democratic': 4928})})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier:  0.5640556919669724\n"
     ]
    }
   ],
   "source": [
    "# Store the predictions in a dataframe\n",
    "predictions = []\n",
    "conv_features_sets = [(conv_features(tweet, feature_words), party) \n",
    "                      for (tweet, party) in tweet_data]\n",
    "for tweet, party in tweet_data_sample:\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "    predictions.append((tweet, party, estimated_party))\n",
    "classifier_df = pd.DataFrame(predictions, \n",
    "                             columns=[\"tweet\", \"actual_party\", \"predicted_party\"])\n",
    "# Print the accuracy of the classifier using nltk.classify.accuracy\n",
    "print(\"Accuracy of the classifier: \", \n",
    "      nltk.classify.accuracy(classifier, conv_features_sets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>actual_party</th>\n",
       "      <th>predicted_party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jimmy panetta</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marcy kaptur</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debbie wasserman schultz</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dave brat</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>antonio sabàto jr</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marcia fudge</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scott peters</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mariah phillips</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jimmy panetta</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lucille</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tweet actual_party predicted_party\n",
       "0             jimmy panetta   Democratic      Democratic\n",
       "1              marcy kaptur   Democratic      Democratic\n",
       "2  debbie wasserman schultz   Democratic      Democratic\n",
       "3                 dave brat   Republican      Republican\n",
       "4         antonio sabàto jr   Republican      Democratic\n",
       "5              marcia fudge   Democratic      Democratic\n",
       "6              scott peters   Democratic      Democratic\n",
       "7           mariah phillips   Democratic      Democratic\n",
       "8             jimmy panetta   Democratic      Democratic\n",
       "9                   lucille   Democratic      Democratic"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "The results of the classifier show that the overall accuracy of the classifier is 56.41%. This means that out of all the tweets the classifier tried to classify, 56.41% were correctly classified. From the results of the classifier, we see that the classifier had a higher accuracy in classifying tweets as 'Democratic' (4928 tweets) than 'Republican' (656 tweets). This could be because of the imbalance in the dataset, where there were more 'Democratic' tweets than 'Republican' tweets. The results also show that there were 3726 'Democratic' tweets classified as 'Republican' and 692 'Republican' tweets classified as 'Democratic.' It could be that the classifier has learned certain words that are common in both the 'Republican' and 'Democratic' tweets, leading to incorrect classifications.\n",
    "\n",
    "Considering the imbalance in the dataset, it is important to test for other metrics such as specificity or recall to get a more comprehensive understanding of the performance of the classifier. Despite this, the accuracy of 56.41% slightly beats a random model (coin toss 50/50) and can be considered a good start for the classifier. However, more work can be done to improve its performance, especially when it comes to handling imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
